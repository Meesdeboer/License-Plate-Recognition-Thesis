{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import easyocr\n",
    "reader = easyocr.Reader(['en'], recognizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOLO-obb-small model trained on synthetic data on Google Colab runtime\n",
    "\"\"\"\n",
    "model = YOLO(\"../models/yolo-obb-light-100ep.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_license_plate(img):\n",
    "    \"\"\"\n",
    "    Runs the YOLO model on the input image and crops the license plate\n",
    "\n",
    "    Returns:\n",
    "    - Image: the cropped license plate\n",
    "    \"\"\"\n",
    "    results = model(img)\n",
    "\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "    xyxyxyxy = detections[0].data['xyxyxyxy'][0]\n",
    "\n",
    "    point1, point2, point3, point4 = xyxyxyxy\n",
    "    box_points = [point3, point2, point1, point4]\n",
    "\n",
    "    if box_points[0][0] > box_points[1][0]:\n",
    "        box_points = [point1, point4, point3, point2]\n",
    "\n",
    "    # make blank image of size 520x110\n",
    "    blank_image = np.zeros((110, 520, 3), np.uint8)\n",
    "    blank_image[:] = (255, 255, 255)\n",
    "\n",
    "    # warp\n",
    "    matrix = cv2.getPerspectiveTransform(np.array(box_points, dtype=np.float32), np.array([[0, 0], [520, 0], [520, 110], [0, 110]], dtype=np.float32))\n",
    "    warped = cv2.warpPerspective(cv2.imread(img), matrix, (520, 110))\n",
    "\n",
    "    warped = cv2.cvtColor(warped, cv2.COLOR_BGR2RGB)\n",
    "    warped = Image.fromarray(warped)\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(img):\n",
    "    \"\"\"\n",
    "    Detects text boxes in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the detected text\n",
    "    \"\"\"\n",
    "    img = np.array(img)\n",
    "    result = reader.detect(img, add_margin=0, slope_ths=0)[1][0]\n",
    "    return result\n",
    "\n",
    "def find_largest_text_area(results):\n",
    "    \"\"\"\n",
    "    Finds the largest text area in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the largest text area\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_width_box = None\n",
    "    for box in results:\n",
    "        width = box[1][0] - box[0][0]\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "            max_width_box = box\n",
    "\n",
    "    return max_width_box\n",
    "\n",
    "def find_largest_text_area(results):\n",
    "    \"\"\"\n",
    "    Finds the largest text area in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the largest text area\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_width_box = None\n",
    "    for box in results:\n",
    "        width = box[1][0] - box[0][0]\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "            max_width_box = box\n",
    "\n",
    "    return max_width_box\n",
    "\n",
    "def extract_box(img, result):\n",
    "    \"\"\"\n",
    "    Extracts the bounding box of the detected text\n",
    "\n",
    "    Returns:\n",
    "    - Image: the cropped text box\n",
    "    \"\"\"\n",
    "    p1, p2, p3, p4 = result\n",
    "    box_points = [p1, p2, p3, p4]\n",
    "\n",
    "    # make blank image of size 520x110\n",
    "    size = (510, 110)\n",
    "    blank_image = np.zeros((size[0], size[1], 3), np.uint8)\n",
    "    blank_image[:] = (255, 255, 255)\n",
    "\n",
    "    # warp\n",
    "    matrix = cv2.getPerspectiveTransform(np.array(box_points, dtype=np.float32), np.array([[0, 0], [size[0], 0], [size[0], size[1]], [0, size[1]]], dtype=np.float32))\n",
    "    warped = cv2.warpPerspective(img, matrix, (size[0], size[1]))\n",
    "\n",
    "    warped = Image.fromarray(warped)\n",
    "\n",
    "    return warped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding(img):\n",
    "    \"\"\"\n",
    "    Applies thresholding to the input image\n",
    "    \"\"\"\n",
    "    img.save(\"temp.jpg\")\n",
    "    img = cv2.imread(\"temp.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    \n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    _, im_gray_th_otsu = cv2.threshold(closing, 128, 192, cv2.THRESH_OTSU)\n",
    "\n",
    "    _, im_gray_th = cv2.threshold(im_gray_th_otsu, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    im_gray_th = cv2.bitwise_not(im_gray_th)\n",
    "\n",
    "    return Image.fromarray(im_gray_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/IMG_2062.jpg: 640x480 575.8ms\n",
      "Speed: 5.4ms preprocess, 575.8ms inference, 20.2ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=510x110>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_binary_lp_cutout(path_to_img):\n",
    "    \"\"\"\n",
    "    Returns a binary image of the license plate cutout\n",
    "    \"\"\"\n",
    "    cropped = crop_license_plate(path_to_img)\n",
    "    result = detect_text(cropped)\n",
    "    result = find_largest_text_area(result)\n",
    "\n",
    "    return thresholding(extract_box(np.array(cropped), result))\n",
    "\n",
    "get_binary_lp_cutout(\"../test_notebooks/images/IMG_2062.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHARACHTER SEGMENATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/IMG_2062.jpg: 640x480 495.2ms\n",
      "Speed: 5.0ms preprocess, 495.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = get_binary_lp_cutout(\"../test_notebooks/images/IMG_2062.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "img_bgr = cv2.cvtColor(np.array(img), cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "import functools\n",
    "# Find contours and get bounding box for each contour\n",
    "cnts, _ = cv2.findContours(np.array(img), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
    "\n",
    "# Keep 8 largest bounding boxes by area\n",
    "boundingBoxes = sorted(boundingBoxes, key=lambda x: x[2]*x[3], reverse=True)[:8]\n",
    "\n",
    "# Sort the bounding boxes from left to right, top to bottom\n",
    "# sort by Y first, and then sort by X if Ys are similar\n",
    "def compare(rect1, rect2):\n",
    "    if abs(rect1[1] - rect2[1]) > 10:\n",
    "        return rect1[1] - rect2[1]\n",
    "    else:\n",
    "        return rect1[0] - rect2[0]\n",
    "boundingBoxes = sorted(boundingBoxes, key=functools.cmp_to_key(compare))\n",
    "\n",
    "# Draw bounding boxes\n",
    "for box in boundingBoxes:\n",
    "    x, y, w, h = box\n",
    "    cv2.rectangle(img_bgr, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "# save image with bounding boxes\n",
    "cv2.imwrite(\"bounding_boxes.jpg\", img_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/5.jpg: 640x480 547.3ms\n",
      "Speed: 8.4ms preprocess, 547.3ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "2 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def do_character_segmentation(img):\n",
    "    \"\"\"\n",
    "    Does character segmentation on the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of cropped character images\n",
    "    - idx: index of the two smallest bounding boxes i.e the two stripes\n",
    "    \"\"\"\n",
    "    img = np.array(img)\n",
    "    cnts, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
    "    boundingBoxes = sorted(boundingBoxes, key=lambda x: x[2]*x[3], reverse=True)[:8]\n",
    "    boundingBoxes = sorted(boundingBoxes, key=lambda x: x[0])\n",
    "\n",
    "    # Get two smallest bounding boxes\n",
    "    boundingBoxesSmallest = sorted(boundingBoxes, key=lambda x: x[2]*x[3])[:2]\n",
    "    \n",
    "    # Get index of the two smallest bounding boxes\n",
    "    idx1 = boundingBoxes.index(boundingBoxesSmallest[0])\n",
    "    idx2 = boundingBoxes.index(boundingBoxesSmallest[1])\n",
    "\n",
    "\n",
    "    characters = []\n",
    "    for box in boundingBoxes:\n",
    "        x, y, w, h = box\n",
    "        characters.append(img[y:y+h, x:x+w])\n",
    "\n",
    "    return characters, (idx1, idx2)\n",
    "\n",
    "characters = do_character_segmentation(np.array(get_binary_lp_cutout(\"../test_notebooks/images/5.jpg\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test trained OCR model on LP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:38:55.323279: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-06 13:38:55.323362: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-06 13:38:55.324527: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['temp.jpg', 'bounding_boxes.jpg', 'full_model.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V']\n",
      "[[ 3.6563e-12  4.1567e-18  4.8324e-12  9.6413e-22  4.6306e-14  1.2486e-22  4.2687e-16  8.1296e-13  4.5604e-18  1.8956e-06  1.2027e-13  1.3208e-17  9.2463e-12  2.9546e-17  2.3108e-14  1.0859e-17  6.0046e-12  1.4075e-19   6.359e-13  9.1988e-13  1.5436e-06  1.9402e-14   7.881e-13           1  1.1287e-09  1.2375e-12\n",
      "   1.0426e-21]]\n",
      "['B']\n",
      "[[ 2.7338e-05  1.5916e-18  2.3542e-08  9.9239e-06   4.818e-07  0.00036926  3.1393e-10  1.6603e-11  5.7409e-05  7.3145e-06     0.62215  5.4571e-05  8.3484e-11  1.7188e-06     0.37686  4.1663e-10  7.9021e-09  1.1889e-13  3.5038e-07   0.0004586  2.0663e-06  1.9899e-10  6.9033e-14  4.2763e-10  6.9656e-12  2.6339e-13\n",
      "   1.0423e-09]]\n",
      "['P']\n",
      "[[ 0.00016814  1.2044e-18  2.4154e-09  8.0614e-08  7.9143e-11  1.4661e-09  8.5037e-16  1.6119e-10  9.9526e-11  7.3212e-09  0.00029799  2.4037e-08   3.263e-12  4.4385e-11  6.0901e-06  5.9011e-09  3.3777e-14   3.525e-18  2.1582e-08     0.99953  3.3569e-07   9.209e-17  5.6098e-15  4.3072e-11  1.1212e-11  4.6404e-14\n",
      "   2.3065e-13]]\n",
      "['6']\n",
      "[[ 9.0089e-14  6.1102e-16  3.4753e-18  1.0185e-11  2.3172e-08   0.0059947     0.99385  2.2853e-18   0.0001093  1.5656e-13  1.9219e-08  4.3799e-10  3.4427e-08  2.1908e-06   2.201e-07  9.7693e-17  4.2227e-09  4.5881e-05  6.9933e-10  3.7033e-10  4.4188e-15  2.2641e-08  5.3765e-17  2.5631e-14  1.0686e-09  1.7131e-13\n",
      "   8.2995e-12]]\n",
      "['3']\n",
      "[[ 1.0234e-22  0.00049915  4.3288e-09     0.99014  0.00010977  1.3253e-12  9.2379e-18     0.00541  2.8516e-11  6.1353e-09  4.4566e-09  1.3036e-15  9.1401e-17  1.2656e-19   9.656e-21   1.004e-05  6.9114e-17   2.028e-24  2.8534e-09  1.1035e-11   5.101e-13  8.2265e-11   0.0038219  1.2246e-14  8.5902e-06  1.4895e-08\n",
      "   1.4465e-11]]\n",
      "['V']\n",
      "[[  2.545e-10  4.4711e-17  8.7795e-11  1.8254e-21  1.1071e-14  2.9128e-23  1.3721e-15  7.0433e-11  1.1733e-17   1.935e-08  6.1297e-13  9.9794e-16  5.9278e-12  3.1145e-17   2.652e-14  1.0112e-15   1.971e-11  2.8758e-18  5.4096e-14  3.5031e-11  3.8267e-06  2.9284e-16  9.9437e-13           1  1.9476e-09  3.8096e-12\n",
      "   6.9724e-19]]\n",
      "['W']\n",
      "[[ 1.1435e-10  3.4867e-10  3.4934e-08  7.8591e-09   0.0011322  2.9932e-11  3.8156e-07  2.2421e-08  2.7911e-08  1.0772e-08    0.018408  4.4413e-08  3.1617e-07  2.1589e-08  4.9193e-12  1.9168e-12  0.00057457  6.9053e-10  5.3461e-05  2.8289e-06  2.1317e-05   7.365e-09   9.554e-09  2.5815e-05     0.97978  5.2564e-07\n",
      "   2.2782e-08]]\n",
      "['W']\n",
      "[[ 8.2265e-11  2.2926e-10  4.4043e-08  6.9458e-09  0.00070154  4.2244e-11  2.7122e-07  3.0032e-08   3.142e-08  1.1005e-08    0.043765  4.4507e-08   5.131e-07  3.0332e-08  2.3965e-12   1.199e-12  0.00051775  6.2378e-10  4.1702e-05   2.118e-06  1.4382e-05  5.5721e-09  1.7118e-08   1.072e-05     0.95495    4.72e-07\n",
      "   2.7397e-08]]\n"
     ]
    }
   ],
   "source": [
    "model = '../models/ocr_model.h5'\n",
    "ocr = keras.models.load_model(model)\n",
    "\n",
    "\n",
    "for box in boundingBoxes:\n",
    "    x, y, w, h = box\n",
    "    cropped = img.crop((x, y, x+w, y+h))\n",
    "    cropped = cropped.resize((20, 30))\n",
    "\n",
    "    # create blank black image\n",
    "    new_img = Image.new(\"RGB\", (23, 33), \"black\")\n",
    "\n",
    "    # paste the original image on the blank image in middle\n",
    "    width, height = cropped.size\n",
    "    x = 23//2 - width//2\n",
    "    y = 33//2 - height//2\n",
    "    new_img.paste(cropped, (x, y))\n",
    "\n",
    "    new_img = new_img.convert('L')\n",
    "\n",
    "    img_array = np.array(new_img)\n",
    "    img_array = img_array / 255.0\n",
    "    img_array = img_array.astype(int)\n",
    "    # print(img_array)\n",
    "    # predict\n",
    "    img_array = img_array.reshape(1, 33, 23, 1)\n",
    "    prediction = ocr.predict(img_array, verbose=0)\n",
    "\n",
    "    lb = pickle.load(open('../test_notebooks/label_encoder.pkl', 'rb'))\n",
    "    prediction = np.array(prediction)\n",
    "    # transform the preidctions using the label binarizer\n",
    "    preds = lb.inverse_transform(prediction.reshape(1, -1))\n",
    "\n",
    "    print(preds)\n",
    "    print(prediction)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'80-TR-VL'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = pickle.load(open('../models/label_encoder.pkl', 'rb'))\n",
    "\n",
    "def predict_characters(characters, idx):\n",
    "    \"\"\"\n",
    "    Predicts the characters in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of predicted characters\n",
    "    \"\"\"\n",
    "    predicted_characters = ['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
    "    for i, char in enumerate(characters):\n",
    "        if i == idx[0] or i == idx[1]:\n",
    "            predicted_characters[i] = '-'\n",
    "            continue\n",
    "\n",
    "        char = Image.fromarray(char)\n",
    "        char = char.resize((20, 30))\n",
    "\n",
    "        new_img = Image.new(\"RGB\", (23, 33), \"black\")\n",
    "\n",
    "        width, height = char.size\n",
    "        x = 23//2 - width//2\n",
    "        y = 33//2 - height//2\n",
    "\n",
    "        new_img.paste(char, (x, y))\n",
    "        new_img = new_img.convert('L')\n",
    "\n",
    "        img_array = np.array(new_img)\n",
    "        img_array = img_array / 255.0\n",
    "        img_array = img_array.astype(int)\n",
    "        img_array = img_array.reshape(1, 33, 23, 1)\n",
    "\n",
    "        prediction = ocr.predict(img_array, verbose=0)\n",
    "        prediction = np.array(prediction)\n",
    "        preds = lb.inverse_transform(prediction.reshape(1, -1))\n",
    "        predicted_characters[i] = preds[0]\n",
    "\n",
    "    predicted_characters = ''.join(predicted_characters)\n",
    "\n",
    "    return predicted_characters\n",
    "        \n",
    "        \n",
    "\n",
    "predict_characters(characters[0], characters[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
