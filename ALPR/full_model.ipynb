{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import easyocr\n",
    "reader = easyocr.Reader(['en'], recognizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOLO-obb-small model trained on synthetic data on Google Colab runtime\n",
    "\"\"\"\n",
    "model = YOLO(\"../models/yolo-obb-s-100ep.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_license_plate(img):\n",
    "    \"\"\"\n",
    "    Runs the YOLO model on the input image and crops the license plate\n",
    "\n",
    "    Returns:\n",
    "    - Image: the cropped license plate\n",
    "    \"\"\"\n",
    "    results = model(img)\n",
    "\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "    xyxyxyxy = detections[0].data['xyxyxyxy'][0]\n",
    "\n",
    "    point1, point2, point3, point4 = xyxyxyxy\n",
    "    box_points = [point3, point2, point1, point4]\n",
    "\n",
    "    if box_points[0][0] > box_points[1][0]:\n",
    "        box_points = [point1, point4, point3, point2]\n",
    "\n",
    "    # make blank image of size 520x110\n",
    "    blank_image = np.zeros((110, 520, 3), np.uint8)\n",
    "    blank_image[:] = (255, 255, 255)\n",
    "\n",
    "    # warp\n",
    "    matrix = cv2.getPerspectiveTransform(np.array(box_points, dtype=np.float32), np.array([[0, 0], [520, 0], [520, 110], [0, 110]], dtype=np.float32))\n",
    "    warped = cv2.warpPerspective(cv2.imread(img), matrix, (520, 110))\n",
    "\n",
    "    warped = cv2.cvtColor(warped, cv2.COLOR_BGR2RGB)\n",
    "    warped = Image.fromarray(warped)\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(img):\n",
    "    \"\"\"\n",
    "    Detects text boxes in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the detected text\n",
    "    \"\"\"\n",
    "    img = np.array(img)\n",
    "    result = reader.detect(img, add_margin=0, slope_ths=0)[1][0]\n",
    "    return result\n",
    "\n",
    "def find_largest_text_area(results):\n",
    "    \"\"\"\n",
    "    Finds the largest text area in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the largest text area\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_width_box = None\n",
    "    for box in results:\n",
    "        width = box[1][0] - box[0][0]\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "            max_width_box = box\n",
    "\n",
    "    return max_width_box\n",
    "\n",
    "def find_largest_text_area(results):\n",
    "    \"\"\"\n",
    "    Finds the largest text area in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the largest text area\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_width_box = None\n",
    "    for box in results:\n",
    "        width = box[1][0] - box[0][0]\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "            max_width_box = box\n",
    "\n",
    "    return max_width_box\n",
    "\n",
    "def extract_box(img, result):\n",
    "    \"\"\"\n",
    "    Extracts the bounding box of the detected text\n",
    "\n",
    "    Returns:\n",
    "    - Image: the cropped text box\n",
    "    \"\"\"\n",
    "    p1, p2, p3, p4 = result\n",
    "    box_points = [p1, p2, p3, p4]\n",
    "\n",
    "    # make blank image of size 520x110\n",
    "    size = (510, 110)\n",
    "    blank_image = np.zeros((size[0], size[1], 3), np.uint8)\n",
    "    blank_image[:] = (255, 255, 255)\n",
    "\n",
    "    # warp\n",
    "    matrix = cv2.getPerspectiveTransform(np.array(box_points, dtype=np.float32), np.array([[0, 0], [size[0], 0], [size[0], size[1]], [0, size[1]]], dtype=np.float32))\n",
    "    warped = cv2.warpPerspective(img, matrix, (size[0], size[1]))\n",
    "\n",
    "    warped = Image.fromarray(warped)\n",
    "\n",
    "    return warped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding(img):\n",
    "    \"\"\"\n",
    "    Applies thresholding to the input image\n",
    "    \"\"\"\n",
    "    img.save(\"temp.jpg\")\n",
    "    img = cv2.imread(\"temp.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    \n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    _, im_gray_th_otsu = cv2.threshold(closing, 128, 192, cv2.THRESH_OTSU)\n",
    "\n",
    "    _, im_gray_th = cv2.threshold(im_gray_th_otsu, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    im_gray_th = cv2.bitwise_not(im_gray_th)\n",
    "\n",
    "    return Image.fromarray(im_gray_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/IMG_2082.jpg: 352x640 479.5ms\n",
      "Speed: 4.0ms preprocess, 479.5ms inference, 13.4ms postprocess per image at shape (1, 3, 352, 640)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=510x110>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_binary_lp_cutout(path_to_img):\n",
    "    \"\"\"\n",
    "    Returns a binary image of the license plate cutout\n",
    "    \"\"\"\n",
    "    cropped = crop_license_plate(path_to_img)\n",
    "    result = detect_text(cropped)\n",
    "    result = find_largest_text_area(result)\n",
    "\n",
    "    return thresholding(extract_box(np.array(cropped), result))\n",
    "\n",
    "get_binary_lp_cutout(\"../test_notebooks/images/IMG_2082.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHARACHTER SEGMENATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/4.jpg: 640x480 507.3ms\n",
      "Speed: 4.6ms preprocess, 507.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = get_binary_lp_cutout(\"../test_notebooks/images/4.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "img_bgr = cv2.cvtColor(np.array(img), cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "import functools\n",
    "# Find contours and get bounding box for each contour\n",
    "cnts, _ = cv2.findContours(np.array(img), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
    "\n",
    "# Keep 8 largest bounding boxes by area\n",
    "boundingBoxes = sorted(boundingBoxes, key=lambda x: x[2]*x[3], reverse=True)[:8]\n",
    "\n",
    "# Sort the bounding boxes from left to right, top to bottom\n",
    "# sort by Y first, and then sort by X if Ys are similar\n",
    "def compare(rect1, rect2):\n",
    "    if abs(rect1[1] - rect2[1]) > 10:\n",
    "        return rect1[1] - rect2[1]\n",
    "    else:\n",
    "        return rect1[0] - rect2[0]\n",
    "boundingBoxes = sorted(boundingBoxes, key=functools.cmp_to_key(compare))\n",
    "\n",
    "# Draw bounding boxes\n",
    "for box in boundingBoxes:\n",
    "    x, y, w, h = box\n",
    "    cv2.rectangle(img_bgr, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "# save image with bounding boxes\n",
    "cv2.imwrite(\"bounding_boxes.jpg\", img_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/5.jpg: 640x480 547.3ms\n",
      "Speed: 8.4ms preprocess, 547.3ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "2 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def do_character_segmentation(img):\n",
    "    \"\"\"\n",
    "    Does character segmentation on the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of cropped character images\n",
    "    - idx: index of the two smallest bounding boxes i.e the two stripes\n",
    "    \"\"\"\n",
    "    img = np.array(img)\n",
    "    cnts, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
    "    boundingBoxes = sorted(boundingBoxes, key=lambda x: x[2]*x[3], reverse=True)[:8]\n",
    "    boundingBoxes = sorted(boundingBoxes, key=lambda x: x[0])\n",
    "\n",
    "    # Get two smallest bounding boxes\n",
    "    boundingBoxesSmallest = sorted(boundingBoxes, key=lambda x: x[2]*x[3])[:2]\n",
    "    \n",
    "    # Get index of the two smallest bounding boxes\n",
    "    idx1 = boundingBoxes.index(boundingBoxesSmallest[0])\n",
    "    idx2 = boundingBoxes.index(boundingBoxesSmallest[1])\n",
    "\n",
    "\n",
    "    characters = []\n",
    "    for box in boundingBoxes:\n",
    "        x, y, w, h = box\n",
    "        characters.append(img[y:y+h, x:x+w])\n",
    "\n",
    "    return characters, (idx1, idx2)\n",
    "\n",
    "characters = do_character_segmentation(np.array(get_binary_lp_cutout(\"../test_notebooks/images/5.jpg\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test trained OCR model on LP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:38:55.323279: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-06 13:38:55.323362: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-06 13:38:55.324527: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['temp.jpg', 'bounding_boxes.jpg', 'full_model.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V']\n",
      "[[ 3.6563e-12  4.1567e-18  4.8324e-12  9.6413e-22  4.6306e-14  1.2486e-22  4.2687e-16  8.1296e-13  4.5604e-18  1.8956e-06  1.2027e-13  1.3208e-17  9.2463e-12  2.9546e-17  2.3108e-14  1.0859e-17  6.0046e-12  1.4075e-19   6.359e-13  9.1988e-13  1.5436e-06  1.9402e-14   7.881e-13           1  1.1287e-09  1.2375e-12\n",
      "   1.0426e-21]]\n",
      "['B']\n",
      "[[ 2.7338e-05  1.5916e-18  2.3542e-08  9.9239e-06   4.818e-07  0.00036926  3.1393e-10  1.6603e-11  5.7409e-05  7.3145e-06     0.62215  5.4571e-05  8.3484e-11  1.7188e-06     0.37686  4.1663e-10  7.9021e-09  1.1889e-13  3.5038e-07   0.0004586  2.0663e-06  1.9899e-10  6.9033e-14  4.2763e-10  6.9656e-12  2.6339e-13\n",
      "   1.0423e-09]]\n",
      "['P']\n",
      "[[ 0.00016814  1.2044e-18  2.4154e-09  8.0614e-08  7.9143e-11  1.4661e-09  8.5037e-16  1.6119e-10  9.9526e-11  7.3212e-09  0.00029799  2.4037e-08   3.263e-12  4.4385e-11  6.0901e-06  5.9011e-09  3.3777e-14   3.525e-18  2.1582e-08     0.99953  3.3569e-07   9.209e-17  5.6098e-15  4.3072e-11  1.1212e-11  4.6404e-14\n",
      "   2.3065e-13]]\n",
      "['6']\n",
      "[[ 9.0089e-14  6.1102e-16  3.4753e-18  1.0185e-11  2.3172e-08   0.0059947     0.99385  2.2853e-18   0.0001093  1.5656e-13  1.9219e-08  4.3799e-10  3.4427e-08  2.1908e-06   2.201e-07  9.7693e-17  4.2227e-09  4.5881e-05  6.9933e-10  3.7033e-10  4.4188e-15  2.2641e-08  5.3765e-17  2.5631e-14  1.0686e-09  1.7131e-13\n",
      "   8.2995e-12]]\n",
      "['3']\n",
      "[[ 1.0234e-22  0.00049915  4.3288e-09     0.99014  0.00010977  1.3253e-12  9.2379e-18     0.00541  2.8516e-11  6.1353e-09  4.4566e-09  1.3036e-15  9.1401e-17  1.2656e-19   9.656e-21   1.004e-05  6.9114e-17   2.028e-24  2.8534e-09  1.1035e-11   5.101e-13  8.2265e-11   0.0038219  1.2246e-14  8.5902e-06  1.4895e-08\n",
      "   1.4465e-11]]\n",
      "['V']\n",
      "[[  2.545e-10  4.4711e-17  8.7795e-11  1.8254e-21  1.1071e-14  2.9128e-23  1.3721e-15  7.0433e-11  1.1733e-17   1.935e-08  6.1297e-13  9.9794e-16  5.9278e-12  3.1145e-17   2.652e-14  1.0112e-15   1.971e-11  2.8758e-18  5.4096e-14  3.5031e-11  3.8267e-06  2.9284e-16  9.9437e-13           1  1.9476e-09  3.8096e-12\n",
      "   6.9724e-19]]\n",
      "['W']\n",
      "[[ 1.1435e-10  3.4867e-10  3.4934e-08  7.8591e-09   0.0011322  2.9932e-11  3.8156e-07  2.2421e-08  2.7911e-08  1.0772e-08    0.018408  4.4413e-08  3.1617e-07  2.1589e-08  4.9193e-12  1.9168e-12  0.00057457  6.9053e-10  5.3461e-05  2.8289e-06  2.1317e-05   7.365e-09   9.554e-09  2.5815e-05     0.97978  5.2564e-07\n",
      "   2.2782e-08]]\n",
      "['W']\n",
      "[[ 8.2265e-11  2.2926e-10  4.4043e-08  6.9458e-09  0.00070154  4.2244e-11  2.7122e-07  3.0032e-08   3.142e-08  1.1005e-08    0.043765  4.4507e-08   5.131e-07  3.0332e-08  2.3965e-12   1.199e-12  0.00051775  6.2378e-10  4.1702e-05   2.118e-06  1.4382e-05  5.5721e-09  1.7118e-08   1.072e-05     0.95495    4.72e-07\n",
      "   2.7397e-08]]\n"
     ]
    }
   ],
   "source": [
    "model = '../models/ocr_model.h5'\n",
    "ocr = keras.models.load_model(model)\n",
    "\n",
    "\n",
    "for box in boundingBoxes:\n",
    "    x, y, w, h = box\n",
    "    cropped = img.crop((x, y, x+w, y+h))\n",
    "    cropped = cropped.resize((20, 30))\n",
    "\n",
    "    # create blank black image\n",
    "    new_img = Image.new(\"RGB\", (23, 33), \"black\")\n",
    "\n",
    "    # paste the original image on the blank image in middle\n",
    "    width, height = cropped.size\n",
    "    x = 23//2 - width//2\n",
    "    y = 33//2 - height//2\n",
    "    new_img.paste(cropped, (x, y))\n",
    "\n",
    "    new_img = new_img.convert('L')\n",
    "\n",
    "    img_array = np.array(new_img)\n",
    "    img_array = img_array / 255.0\n",
    "    img_array = img_array.astype(int)\n",
    "    # print(img_array)\n",
    "    # predict\n",
    "    img_array = img_array.reshape(1, 33, 23, 1)\n",
    "    prediction = ocr.predict(img_array, verbose=0)\n",
    "\n",
    "    lb = pickle.load(open('../test_notebooks/label_encoder.pkl', 'rb'))\n",
    "    prediction = np.array(prediction)\n",
    "    # transform the preidctions using the label binarizer\n",
    "    preds = lb.inverse_transform(prediction.reshape(1, -1))\n",
    "\n",
    "    print(preds)\n",
    "    print(prediction)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'80-TR-VL'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = pickle.load(open('../models/label_encoder.pkl', 'rb'))\n",
    "\n",
    "def predict_characters(characters, idx):\n",
    "    \"\"\"\n",
    "    Predicts the characters in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of predicted characters\n",
    "    \"\"\"\n",
    "    predicted_characters = ['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X']\n",
    "    for i, char in enumerate(characters):\n",
    "        if i == idx[0] or i == idx[1]:\n",
    "            predicted_characters[i] = '-'\n",
    "            continue\n",
    "\n",
    "        char = Image.fromarray(char)\n",
    "        char = char.resize((20, 30))\n",
    "\n",
    "        new_img = Image.new(\"RGB\", (23, 33), \"black\")\n",
    "\n",
    "        width, height = char.size\n",
    "        x = 23//2 - width//2\n",
    "        y = 33//2 - height//2\n",
    "\n",
    "        new_img.paste(char, (x, y))\n",
    "        new_img = new_img.convert('L')\n",
    "\n",
    "        img_array = np.array(new_img)\n",
    "        img_array = img_array / 255.0\n",
    "        img_array = img_array.astype(int)\n",
    "        img_array = img_array.reshape(1, 33, 23, 1)\n",
    "\n",
    "        prediction = ocr.predict(img_array, verbose=0)\n",
    "        prediction = np.array(prediction)\n",
    "        preds = lb.inverse_transform(prediction.reshape(1, -1))\n",
    "        predicted_characters[i] = preds[0]\n",
    "\n",
    "    predicted_characters = ''.join(predicted_characters)\n",
    "\n",
    "    return predicted_characters\n",
    "        \n",
    "        \n",
    "\n",
    "predict_characters(characters[0], characters[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
