{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import easyocr\n",
    "reader = easyocr.Reader(['en'], recognizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOLO-obb-small model trained on synthetic data on Google Colab runtime\n",
    "\"\"\"\n",
    "model = YOLO(\"../models/yolo-obb-s-100ep.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_license_plate(img):\n",
    "    \"\"\"\n",
    "    Runs the YOLO model on the input image and crops the license plate\n",
    "\n",
    "    Returns:\n",
    "    - Image: the cropped license plate\n",
    "    \"\"\"\n",
    "    results = model(img)\n",
    "\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "    xyxyxyxy = detections[0].data['xyxyxyxy'][0]\n",
    "\n",
    "    point1, point2, point3, point4 = xyxyxyxy\n",
    "    box_points = [point3, point2, point1, point4]\n",
    "\n",
    "    if box_points[0][0] > box_points[1][0]:\n",
    "        box_points = [point1, point4, point3, point2]\n",
    "\n",
    "    # make blank image of size 520x110\n",
    "    blank_image = np.zeros((110, 520, 3), np.uint8)\n",
    "    blank_image[:] = (255, 255, 255)\n",
    "\n",
    "    # warp\n",
    "    matrix = cv2.getPerspectiveTransform(np.array(box_points, dtype=np.float32), np.array([[0, 0], [520, 0], [520, 110], [0, 110]], dtype=np.float32))\n",
    "    warped = cv2.warpPerspective(cv2.imread(img), matrix, (520, 110))\n",
    "\n",
    "    warped = cv2.cvtColor(warped, cv2.COLOR_BGR2RGB)\n",
    "    warped = Image.fromarray(warped)\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(img):\n",
    "    \"\"\"\n",
    "    Detects text boxes in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the detected text\n",
    "    \"\"\"\n",
    "    img = np.array(img)\n",
    "    result = reader.detect(img, add_margin=0, slope_ths=0)[1][0]\n",
    "    return result\n",
    "\n",
    "def find_largest_text_area(results):\n",
    "    \"\"\"\n",
    "    Finds the largest text area in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the largest text area\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_width_box = None\n",
    "    for box in results:\n",
    "        width = box[1][0] - box[0][0]\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "            max_width_box = box\n",
    "\n",
    "    return max_width_box\n",
    "\n",
    "def find_largest_text_area(results):\n",
    "    \"\"\"\n",
    "    Finds the largest text area in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the largest text area\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_width_box = None\n",
    "    for box in results:\n",
    "        width = box[1][0] - box[0][0]\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "            max_width_box = box\n",
    "\n",
    "    return max_width_box\n",
    "\n",
    "def extract_box(img, result):\n",
    "    \"\"\"\n",
    "    Extracts the bounding box of the detected text\n",
    "\n",
    "    Returns:\n",
    "    - Image: the cropped text box\n",
    "    \"\"\"\n",
    "    p1, p2, p3, p4 = result\n",
    "    box_points = [p1, p2, p3, p4]\n",
    "\n",
    "    # make blank image of size 520x110\n",
    "    size = (510, 110)\n",
    "    blank_image = np.zeros((size[0], size[1], 3), np.uint8)\n",
    "    blank_image[:] = (255, 255, 255)\n",
    "\n",
    "    # warp\n",
    "    matrix = cv2.getPerspectiveTransform(np.array(box_points, dtype=np.float32), np.array([[0, 0], [size[0], 0], [size[0], size[1]], [0, size[1]]], dtype=np.float32))\n",
    "    warped = cv2.warpPerspective(img, matrix, (size[0], size[1]))\n",
    "\n",
    "    warped = Image.fromarray(warped)\n",
    "\n",
    "    return warped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding(img):\n",
    "    \"\"\"\n",
    "    Applies thresholding to the input image\n",
    "    \"\"\"\n",
    "    img.save(\"temp.jpg\")\n",
    "    img = cv2.imread(\"temp.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    \n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    _, im_gray_th_otsu = cv2.threshold(closing, 128, 192, cv2.THRESH_OTSU)\n",
    "\n",
    "    _, im_gray_th = cv2.threshold(im_gray_th_otsu, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    im_gray_th = cv2.bitwise_not(im_gray_th)\n",
    "\n",
    "    return Image.fromarray(im_gray_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/11.jpg: 640x480 407.6ms\n",
      "Speed: 3.8ms preprocess, 407.6ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=510x110>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_binary_lp_cutout(path_to_img):\n",
    "    \"\"\"\n",
    "    Returns a binary image of the license plate cutout\n",
    "    \"\"\"\n",
    "    cropped = crop_license_plate(path_to_img)\n",
    "    result = detect_text(cropped)\n",
    "    result = find_largest_text_area(result)\n",
    "\n",
    "    return thresholding(extract_box(np.array(cropped), result))\n",
    "\n",
    "get_binary_lp_cutout(\"../test_notebooks/images/11.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHARACHTER SEGMENATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/11.jpg: 640x480 436.8ms\n",
      "Speed: 4.9ms preprocess, 436.8ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = get_binary_lp_cutout(\"../test_notebooks/images/11.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "img_bgr = cv2.cvtColor(np.array(img), cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "import functools\n",
    "# Find contours and get bounding box for each contour\n",
    "cnts, _ = cv2.findContours(np.array(img), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
    "\n",
    "# Keep 8 largest bounding boxes by area\n",
    "boundingBoxes = sorted(boundingBoxes, key=lambda x: x[2]*x[3], reverse=True)[:8]\n",
    "\n",
    "# Sort the bounding boxes from left to right, top to bottom\n",
    "# sort by Y first, and then sort by X if Ys are similar\n",
    "def compare(rect1, rect2):\n",
    "    if abs(rect1[1] - rect2[1]) > 10:\n",
    "        return rect1[1] - rect2[1]\n",
    "    else:\n",
    "        return rect1[0] - rect2[0]\n",
    "boundingBoxes = sorted(boundingBoxes, key=functools.cmp_to_key(compare))\n",
    "\n",
    "# Draw bounding boxes\n",
    "for box in boundingBoxes:\n",
    "    x, y, w, h = box\n",
    "    cv2.rectangle(img_bgr, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "# save image with bounding boxes\n",
    "cv2.imwrite(\"bounding_boxes.jpg\", img_bgr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
