{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import easyocr\n",
    "reader = easyocr.Reader(['en'], recognizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOLO-obb-small model trained on synthetic data on Google Colab runtime\n",
    "\"\"\"\n",
    "model = YOLO(\"../models/yolo-obb-s-100ep.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_license_plate(img):\n",
    "    \"\"\"\n",
    "    Runs the YOLO model on the input image and crops the license plate\n",
    "\n",
    "    Returns:\n",
    "    - Image: the cropped license plate\n",
    "    \"\"\"\n",
    "    results = model(img)\n",
    "\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "    xyxyxyxy = detections[0].data['xyxyxyxy'][0]\n",
    "\n",
    "    point1, point2, point3, point4 = xyxyxyxy\n",
    "    box_points = [point3, point2, point1, point4]\n",
    "\n",
    "    # make blank image of size 520x110\n",
    "    blank_image = np.zeros((110, 520, 3), np.uint8)\n",
    "    blank_image[:] = (255, 255, 255)\n",
    "\n",
    "    # warp\n",
    "    matrix = cv2.getPerspectiveTransform(np.array(box_points, dtype=np.float32), np.array([[0, 0], [520, 0], [520, 110], [0, 110]], dtype=np.float32))\n",
    "    warped = cv2.warpPerspective(cv2.imread(img), matrix, (520, 110))\n",
    "\n",
    "    warped = cv2.cvtColor(warped, cv2.COLOR_BGR2RGB)\n",
    "    warped = Image.fromarray(warped)\n",
    "    return warped\n",
    "\n",
    "def correct_cropped_license_plate(img):\n",
    "    img = crop_license_plate(img)\n",
    "    cropped = np.array(img)\n",
    "    cropped = cropped[:, :, 2]\n",
    "    sums = np.sum(cropped, axis=0)\n",
    "    left = sums[:260]\n",
    "    right = sums[260:]\n",
    "    total_sum_left = np.sum(left)\n",
    "    total_sum_right = np.sum(right)\n",
    "\n",
    "    if total_sum_left < total_sum_right:\n",
    "        flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        return flipped_img\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(img):\n",
    "    \"\"\"\n",
    "    Detects text boxes in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the detected text\n",
    "    \"\"\"\n",
    "    img = np.array(img)\n",
    "    result = reader.detect(img, add_margin=0, slope_ths=0)[1][0]\n",
    "    return result\n",
    "\n",
    "def find_largest_text_area(results):\n",
    "    \"\"\"\n",
    "    Finds the largest text area in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the largest text area\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_width_box = None\n",
    "    for box in results:\n",
    "        width = box[1][0] - box[0][0]\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "            max_width_box = box\n",
    "\n",
    "    return max_width_box\n",
    "\n",
    "def find_largest_text_area(results):\n",
    "    \"\"\"\n",
    "    Finds the largest text area in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the largest text area\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_width_box = None\n",
    "    for box in results:\n",
    "        width = box[1][0] - box[0][0]\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "            max_width_box = box\n",
    "\n",
    "    return max_width_box\n",
    "\n",
    "def extract_box(img, result):\n",
    "    \"\"\"\n",
    "    Extracts the bounding box of the detected text\n",
    "\n",
    "    Returns:\n",
    "    - Image: the cropped text box\n",
    "    \"\"\"\n",
    "    p1, p2, p3, p4 = result\n",
    "    box_points = [p1, p2, p3, p4]\n",
    "\n",
    "    # make blank image of size 520x110\n",
    "    size = (510, 110)\n",
    "    blank_image = np.zeros((size[0], size[1], 3), np.uint8)\n",
    "    blank_image[:] = (255, 255, 255)\n",
    "\n",
    "    # warp\n",
    "    matrix = cv2.getPerspectiveTransform(np.array(box_points, dtype=np.float32), np.array([[0, 0], [size[0], 0], [size[0], size[1]], [0, size[1]]], dtype=np.float32))\n",
    "    warped = cv2.warpPerspective(img, matrix, (size[0], size[1]))\n",
    "\n",
    "    warped = Image.fromarray(warped)\n",
    "\n",
    "    return warped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding(img):\n",
    "    \"\"\"\n",
    "    Applies thresholding to the input image\n",
    "    \"\"\"\n",
    "    img.save(\"temp.jpg\")\n",
    "    img = cv2.imread(\"temp.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    \n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    _, im_gray_th_otsu = cv2.threshold(closing, 128, 192, cv2.THRESH_OTSU)\n",
    "\n",
    "    _, im_gray_th = cv2.threshold(im_gray_th_otsu, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    im_gray_th = cv2.bitwise_not(im_gray_th)\n",
    "\n",
    "    return Image.fromarray(im_gray_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/IMG_2063.jpg: 640x480 547.6ms\n",
      "Speed: 9.0ms preprocess, 547.6ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=510x110>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_binary_lp_cutout(path_to_img):\n",
    "    \"\"\"\n",
    "    Returns a binary image of the license plate cutout\n",
    "    \"\"\"\n",
    "    cropped = correct_cropped_license_plate(path_to_img)\n",
    "    result = detect_text(cropped)\n",
    "    result = find_largest_text_area(result)\n",
    "\n",
    "    return thresholding(extract_box(np.array(cropped), result))\n",
    "\n",
    "get_binary_lp_cutout(\"../test_notebooks/images/IMG_2063.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHARACHTER SEGMENATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/IMG_2063.jpg: 640x480 435.8ms\n",
      "Speed: 3.0ms preprocess, 435.8ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = get_binary_lp_cutout(\"../test_notebooks/images/IMG_2063.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "img_bgr = cv2.cvtColor(np.array(img), cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "import functools\n",
    "# Find contours and get bounding box for each contour\n",
    "cnts, _ = cv2.findContours(np.array(img), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
    "\n",
    "# Keep 8 largest bounding boxes by area\n",
    "boundingBoxes = sorted(boundingBoxes, key=lambda x: x[2]*x[3], reverse=True)[:8]\n",
    "\n",
    "# Sort the bounding boxes from left to right, top to bottom\n",
    "# sort by Y first, and then sort by X if Ys are similar\n",
    "def compare(rect1, rect2):\n",
    "    if abs(rect1[1] - rect2[1]) > 10:\n",
    "        return rect1[1] - rect2[1]\n",
    "    else:\n",
    "        return rect1[0] - rect2[0]\n",
    "boundingBoxes = sorted(boundingBoxes, key=functools.cmp_to_key(compare))\n",
    "\n",
    "# Draw bounding boxes\n",
    "for box in boundingBoxes:\n",
    "    x, y, w, h = box\n",
    "    cv2.rectangle(img_bgr, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "# save image with bounding boxes\n",
    "cv2.imwrite(\"bounding_boxes.jpg\", img_bgr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
