{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import easyocr\n",
    "reader = easyocr.Reader(['en'], recognizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOLO-obb-small model trained on synthetic data on Google Colab runtime\n",
    "\"\"\"\n",
    "model = YOLO(\"../models/yolo-obb-s-100ep.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_license_plate(img):\n",
    "    \"\"\"\n",
    "    Runs the YOLO model on the input image and crops the license plate\n",
    "\n",
    "    Returns:\n",
    "    - Image: the cropped license plate\n",
    "    \"\"\"\n",
    "    results = model(img)\n",
    "\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "    xyxyxyxy = detections[0].data['xyxyxyxy'][0]\n",
    "\n",
    "    point1, point2, point3, point4 = xyxyxyxy\n",
    "    box_points = [point3, point2, point1, point4]\n",
    "\n",
    "    # make blank image of size 520x110\n",
    "    blank_image = np.zeros((110, 520, 3), np.uint8)\n",
    "    blank_image[:] = (255, 255, 255)\n",
    "\n",
    "    # warp\n",
    "    matrix = cv2.getPerspectiveTransform(np.array(box_points, dtype=np.float32), np.array([[0, 0], [520, 0], [520, 110], [0, 110]], dtype=np.float32))\n",
    "    warped = cv2.warpPerspective(cv2.imread(img), matrix, (520, 110))\n",
    "\n",
    "    warped = cv2.cvtColor(warped, cv2.COLOR_BGR2RGB)\n",
    "    warped = Image.fromarray(warped)\n",
    "    return warped\n",
    "\n",
    "def correct_cropped_license_plate(img):\n",
    "    img = crop_license_plate(img)\n",
    "    cropped = np.array(img)\n",
    "    cropped = cropped[:, :, 2]\n",
    "    sums = np.sum(cropped, axis=0)\n",
    "    left = sums[:260]\n",
    "    right = sums[260:]\n",
    "    total_sum_left = np.sum(left)\n",
    "    total_sum_right = np.sum(right)\n",
    "\n",
    "    if total_sum_left < total_sum_right:\n",
    "        flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        return flipped_img\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(img):\n",
    "    \"\"\"\n",
    "    Detects text boxes in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the detected text\n",
    "    \"\"\"\n",
    "    img = np.array(img)\n",
    "    result = reader.detect(img, add_margin=0, slope_ths=0)[1][0]\n",
    "    return result\n",
    "\n",
    "def find_largest_text_area(results):\n",
    "    \"\"\"\n",
    "    Finds the largest text area in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the largest text area\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_width_box = None\n",
    "    for box in results:\n",
    "        width = box[1][0] - box[0][0]\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "            max_width_box = box\n",
    "\n",
    "    return max_width_box\n",
    "\n",
    "def find_largest_text_area(results):\n",
    "    \"\"\"\n",
    "    Finds the largest text area in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the largest text area\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_width_box = None\n",
    "    for box in results:\n",
    "        width = box[1][0] - box[0][0]\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "            max_width_box = box\n",
    "\n",
    "    return max_width_box\n",
    "\n",
    "def extract_box(img, result):\n",
    "    \"\"\"\n",
    "    Extracts the bounding box of the detected text\n",
    "\n",
    "    Returns:\n",
    "    - Image: the cropped text box\n",
    "    \"\"\"\n",
    "    p1, p2, p3, p4 = result\n",
    "    box_points = [p1, p2, p3, p4]\n",
    "\n",
    "    # make blank image of size 520x110\n",
    "    size = (510, 110)\n",
    "    blank_image = np.zeros((size[0], size[1], 3), np.uint8)\n",
    "    blank_image[:] = (255, 255, 255)\n",
    "\n",
    "    # warp\n",
    "    matrix = cv2.getPerspectiveTransform(np.array(box_points, dtype=np.float32), np.array([[0, 0], [size[0], 0], [size[0], size[1]], [0, size[1]]], dtype=np.float32))\n",
    "    warped = cv2.warpPerspective(img, matrix, (size[0], size[1]))\n",
    "\n",
    "    warped = Image.fromarray(warped)\n",
    "\n",
    "    return warped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding(img):\n",
    "    \"\"\"\n",
    "    Applies thresholding to the input image\n",
    "    \"\"\"\n",
    "    img.save(\"temp.jpg\")\n",
    "    img = cv2.imread(\"temp.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    \n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    _, im_gray_th_otsu = cv2.threshold(closing, 128, 192, cv2.THRESH_OTSU)\n",
    "\n",
    "    _, im_gray_th = cv2.threshold(im_gray_th_otsu, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    return Image.fromarray(im_gray_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/IMG_2079.jpg: 640x480 389.4ms\n",
      "Speed: 5.2ms preprocess, 389.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=510x110>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_binary_lp_cutout(path_to_img):\n",
    "    \"\"\"\n",
    "    Returns a binary image of the license plate cutout\n",
    "    \"\"\"\n",
    "    cropped = correct_cropped_license_plate(path_to_img)\n",
    "    result = detect_text(cropped)\n",
    "    result = find_largest_text_area(result)\n",
    "\n",
    "    return thresholding(extract_box(np.array(cropped), result))\n",
    "\n",
    "get_binary_lp_cutout(\"../test_notebooks/images/IMG_2079.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
