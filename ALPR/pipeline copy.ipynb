{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import easyocr\n",
    "reader = easyocr.Reader(['en'], recognizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOLO-obb-small model trained on synthetic data on Google Colab runtime\n",
    "\"\"\"\n",
    "model = YOLO(\"../models/yolo-obb_s_50.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_license_plate(img):\n",
    "    \"\"\"\n",
    "    Runs the YOLO model on the input image and crops the license plate\n",
    "\n",
    "    Returns:\n",
    "    - Image: the cropped license plate\n",
    "    \"\"\"\n",
    "    results = model(img)\n",
    "\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "    xyxyxyxy = detections[0].data['xyxyxyxy'][0]\n",
    "\n",
    "    point1, point2, point3, point4 = xyxyxyxy\n",
    "    box_points = [point3, point2, point1, point4]\n",
    "\n",
    "    # make blank image of size 520x110\n",
    "    blank_image = np.zeros((110, 520, 3), np.uint8)\n",
    "    blank_image[:] = (255, 255, 255)\n",
    "\n",
    "    # warp\n",
    "    matrix = cv2.getPerspectiveTransform(np.array(box_points, dtype=np.float32), np.array([[0, 0], [520, 0], [520, 110], [0, 110]], dtype=np.float32))\n",
    "    warped = cv2.warpPerspective(cv2.imread(img), matrix, (520, 110))\n",
    "\n",
    "    warped = cv2.cvtColor(warped, cv2.COLOR_BGR2RGB)\n",
    "    warped = Image.fromarray(warped)\n",
    "    return warped\n",
    "\n",
    "def correct_cropped_license_plate(img):\n",
    "    img = crop_license_plate(img)\n",
    "    cropped = np.array(img)\n",
    "    cropped = cropped[:, :, 2]\n",
    "    sums = np.sum(cropped, axis=0)\n",
    "    left = sums[:260]\n",
    "    right = sums[260:]\n",
    "    total_sum_left = np.sum(left)\n",
    "    total_sum_right = np.sum(right)\n",
    "\n",
    "    if total_sum_left < total_sum_right:\n",
    "        flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        return flipped_img\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(img):\n",
    "    \"\"\"\n",
    "    Detects text boxes in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the detected text\n",
    "    \"\"\"\n",
    "    img = np.array(img)\n",
    "    result = reader.detect(img, add_margin=0, slope_ths=0)[1][0]\n",
    "    return result\n",
    "\n",
    "def find_largest_text_area(results):\n",
    "    \"\"\"\n",
    "    Finds the largest text area in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the largest text area\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_width_box = None\n",
    "    for box in results:\n",
    "        width = box[1][0] - box[0][0]\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "            max_width_box = box\n",
    "\n",
    "    return max_width_box\n",
    "\n",
    "def find_largest_text_area(results):\n",
    "    \"\"\"\n",
    "    Finds the largest text area in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the largest text area\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_width_box = None\n",
    "    for box in results:\n",
    "        width = box[1][0] - box[0][0]\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "            max_width_box = box\n",
    "\n",
    "    return max_width_box\n",
    "\n",
    "def extract_box(img, result):\n",
    "    \"\"\"\n",
    "    Extracts the bounding box of the detected text\n",
    "\n",
    "    Returns:\n",
    "    - Image: the cropped text box\n",
    "    \"\"\"\n",
    "    p1, p2, p3, p4 = result\n",
    "    box_points = [p1, p2, p3, p4]\n",
    "\n",
    "    # make blank image of size 520x110\n",
    "    size = (510, 110)\n",
    "    blank_image = np.zeros((size[0], size[1], 3), np.uint8)\n",
    "    blank_image[:] = (255, 255, 255)\n",
    "\n",
    "    # warp\n",
    "    matrix = cv2.getPerspectiveTransform(np.array(box_points, dtype=np.float32), np.array([[0, 0], [size[0], 0], [size[0], size[1]], [0, size[1]]], dtype=np.float32))\n",
    "    warped = cv2.warpPerspective(img, matrix, (size[0], size[1]))\n",
    "\n",
    "    warped = Image.fromarray(warped)\n",
    "\n",
    "    return warped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding(img):\n",
    "    \"\"\"\n",
    "    Applies thresholding to the input image\n",
    "    \"\"\"\n",
    "    img.save(\"temp.jpg\")\n",
    "    img = cv2.imread(\"temp.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    \n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    _, im_gray_th_otsu = cv2.threshold(closing, 128, 192, cv2.THRESH_OTSU)\n",
    "\n",
    "    _, im_gray_th = cv2.threshold(im_gray_th_otsu, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    return Image.fromarray(im_gray_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/IMG_2079.jpg: 640x480 628.5ms\n",
      "Speed: 6.6ms preprocess, 628.5ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "[[111.0, 19.0], [463.0, 15.0], [464.0, 90.0], [112.0, 94.0]]\n",
      "\n",
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/IMG_2079.jpg: 640x480 489.2ms\n",
      "Speed: 8.2ms preprocess, 489.2ms inference, 5.4ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=510x110>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = detect_text(correct_cropped_license_plate(\"../test_notebooks/images/IMG_2079.jpg\"))\n",
    "result = find_largest_text_area(result)\n",
    "print(result)\n",
    "thresholding(extract_box(np.array(correct_cropped_license_plate(\"../test_notebooks/images/IMG_2079.jpg\")), result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
