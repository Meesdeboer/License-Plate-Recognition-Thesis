{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import easyocr\n",
    "reader = easyocr.Reader(['en'], recognizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOLO-obb-small model trained on synthetic data on Google Colab runtime\n",
    "\"\"\"\n",
    "model = YOLO(\"../models/yolo-obb_s_50.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_license_plate(img):\n",
    "    \"\"\"\n",
    "    Runs the YOLO model on the input image and crops the license plate\n",
    "\n",
    "    Returns:\n",
    "    - Image: the cropped license plate\n",
    "    \"\"\"\n",
    "    results = model(img)\n",
    "\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "    xyxyxyxy = detections[0].data['xyxyxyxy'][0]\n",
    "\n",
    "    point1, point2, point3, point4 = xyxyxyxy\n",
    "    box_points = [point3, point2, point1, point4]\n",
    "\n",
    "    # make blank image of size 520x110\n",
    "    blank_image = np.zeros((110, 520, 3), np.uint8)\n",
    "    blank_image[:] = (255, 255, 255)\n",
    "\n",
    "    # warp\n",
    "    matrix = cv2.getPerspectiveTransform(np.array(box_points, dtype=np.float32), np.array([[0, 0], [520, 0], [520, 110], [0, 110]], dtype=np.float32))\n",
    "    warped = cv2.warpPerspective(cv2.imread(img), matrix, (520, 110))\n",
    "\n",
    "    warped = cv2.cvtColor(warped, cv2.COLOR_BGR2RGB)\n",
    "    warped = Image.fromarray(warped)\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@2075.669] global loadsave.cpp:248 findDecoder imread_('IMG_2063.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid input type. Supporting format = string(file path or url), bytes, numpy array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 40\u001b[0m\n\u001b[1;32m     35\u001b[0m     cropped_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(cropped_img)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cropped_img\n\u001b[0;32m---> 40\u001b[0m result \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mdetect(cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMG_2063.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m), add_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, slope_ths\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/easyocr/easyocr.py:319\u001b[0m, in \u001b[0;36mReader.detect\u001b[0;34m(self, img, min_size, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, add_margin, reformat, optimal_num_chars, threshold, bbox_min_score, bbox_min_size, max_candidates)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, min_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, text_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m, low_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m,\\\n\u001b[1;32m    312\u001b[0m            link_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m,canvas_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2560\u001b[39m, mag_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m,\\\n\u001b[1;32m    313\u001b[0m            slope_ths \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, ycenter_ths \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, height_ths \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\\\n\u001b[1;32m    314\u001b[0m            width_ths \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, add_margin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, reformat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, optimal_num_chars\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m            threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, bbox_min_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, bbox_min_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, max_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    316\u001b[0m            ):\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reformat:\n\u001b[0;32m--> 319\u001b[0m         img, img_cv_grey \u001b[38;5;241m=\u001b[39m reformat_input(img)\n\u001b[1;32m    321\u001b[0m     text_box_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_textbox(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector, \n\u001b[1;32m    322\u001b[0m                                 img, \n\u001b[1;32m    323\u001b[0m                                 canvas_size \u001b[38;5;241m=\u001b[39m canvas_size, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    334\u001b[0m                                 max_candidates \u001b[38;5;241m=\u001b[39m max_candidates,\n\u001b[1;32m    335\u001b[0m                                 )\n\u001b[1;32m    337\u001b[0m     horizontal_list_agg, free_list_agg \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/easyocr/utils.py:767\u001b[0m, in \u001b[0;36mreformat_input\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    765\u001b[0m     img_cv_grey \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid input type. Supporting format = string(file path or url), bytes, numpy array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, img_cv_grey\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid input type. Supporting format = string(file path or url), bytes, numpy array"
     ]
    }
   ],
   "source": [
    "def detect_text(img):\n",
    "    \"\"\"\n",
    "    Detects text boxes in the input image\n",
    "\n",
    "    Returns:\n",
    "    - list: list of bounding boxes of the detected text\n",
    "    \"\"\"\n",
    "    img = np.array(img)\n",
    "    result = reader.detect(img, add_margin=0)[0][0]\n",
    "    return result\n",
    "\n",
    "def find_largest_text_area(bounding_boxes):\n",
    "    \"\"\"\n",
    "    Finds the largest text area in the input image and returns the bounding box\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    box = None\n",
    "    for box in bounding_boxes:\n",
    "        width = box[1] - box[0]\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "            box = box\n",
    "    return box\n",
    "\n",
    "def extract_license_plate_text(img):\n",
    "    \"\"\"\n",
    "    Extracts the text part of the license plate from the input image\n",
    "    \"\"\"\n",
    "    result = detect_text(img)\n",
    "    box = find_largest_text_area(result)\n",
    "\n",
    "    img = np.array(img)\n",
    "\n",
    "    cropped_img = img[box[2]:box[3], box[0]:box[1]]\n",
    "    cropped_img = Image.fromarray(cropped_img)\n",
    "\n",
    "    return cropped_img\n",
    "\n",
    "\n",
    "result = reader.detect(cv2.imread(\"IMG_2080.jpg\"), add_margin=0, slope_ths=0)[1][0][-1]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/mees/Desktop/UVA/Scriptie/License-Plate-Recognition-Thesis/ALPR/../test_notebooks/images/seat.webp: 384x640 773.7ms\n",
      "Speed: 14.3ms preprocess, 773.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAABPCAAAAAAPCadgAAAFNElEQVR4nO1c2XbjIAwFn/7/L7sPiV0btKLL4jT3YdpMjdCGBEJO3tOBnCjs5P+6cSVuInnnxsvF32gQ/+OwzWbgiwJfi6yGfK5qOmglzLp3xiCCFwcX5eigAKxibJSF4SSp0yLSwLhNfBZheLGxQQ5ul0BTaKMsPKHDIspArJvJ1EI+CV/pBo1KpM0GOclstoEOwlFIU6ls8A/kfiJgSb8sopMMTeoZLD+rUBL/3NGrkKQ3NEECOIMo/hixVwzAZbKloSHpBSG29CGLoD6K9oDzCFgPLDnDPE8wyZY7pjw3TKxE+O0pLGjb/cNRgbHuIGR8NJNizncsmi8vaovs54+bjJjpfODOShQvVoP0lANCu7LIfv0V4HcBEvv5s2V37jHnSqgYLvnN3B9aJ+BpCSd7tVTFP8Daygi+AGSqvrnKFSkZ9lp7zKVo73YvHC8T15XebU0QlAFBpbQIwX4/kUTswqdKctET2WUfhkEzbuVRe6041RNzNkAlv5CEyMwEp9z3hAhkF7lQl65w/ZM7xH6BF065q0WgvngXPQufvKM9I/1/96K0yPyT74F1OBmLao0AFSGQAl+9jkN/P9n2cnP7ob7ZHragE+sg8giK2yidD3UNDT8ppb2QPbc1WMSDTsHIvQIlNst8kPXoE6JuEx2IKiXPBS7njE1pep1zY+Zf4x4rk796hjU+625TxmFjJ3XbxHVEMLZ9H0ys4SBD8M7s9K1cQA2NQ4lqas7ExXNduULMbkF93wKeQD6zj7+EX+VMYpZbv/9winRkdna3gkjyHpi2TTVL4jYtpTl9A00491p8XbnFJqTn2GTB7GQn3d0aGpO1Ry5Ri380mOSnILs2GgvhmkeEy8KYQCP8Vb5hDfE/drndT4hC6Nptz70eNkwMDyt1tDOZYbnjfnlmZ3VdZJNV9kQXLKfbA3fGGC5Phda7X1bXrRvCDjCeL3kM6VVx4cx7RF2r7GYcDFvvL1KNa5jkAF1ppGOXUQ3W7t33TK2jybPY7MAF8BSeBCUc8N1UhihbL1ELKa7ZI22NfsrGg9jrB19FoeJrdxesmkVPJoyLyaZqpyd7Hm9X0XukVNean/OuDFTcMCaZzXT46lQRQG2BVkfouNDk254dzHh7hXQIbLW0YzN4jdP6tcZ6nMK5kRl1mUA3ar37teoZ7x/nbmas0lcNNj1hDKdW6D2N8+KyOrPYDLGPMwvUJIbe+KIIMDVzOisl9HtWeAmQBZxnd2LPiaF6Mm/h61jRlvdHli3htaFhiUQ1sMt7yBuetkZmnzY44Pg61kg2k9USyfzi0gIIZNu3RfL7X4ip4/X89+Mgx3OEjAVQ5JG+nSdlOJ6hnF5zwpJt/f7IQjEHw8pCAllAva0wzyjLRpTo2ycOwei91t0m85wMMfOzsgi/+70sFKBB3PqId9k+zSDSCVFrmuiBKj+u/r0yJ1xvSEhCSb3xSyDYmviwtJ6M38JxRWfbkX1w7F27ilHbbaDh3RaZAVZe5/uSAYOYuuAgM3nrWt3D27rxcxAOixivd6L6Muzr2zoTSUwrEURmur6tsIh7wth4XlZPqYhaQ1sG4rN07LqKIfTqW5FHZJvMurdue2rhWpDEGvVdpjOOhsVUAG36O83WAPMNzLVGxgqkX3t5V8gz0npizyN7ShehptxjyCYZzpJ12YYZk06IPaS2xyPxSTdra6VACcPP7A6TrJDRfBAYM8s9voriUScqoyFMqKoU1RWwrL9dMTWjAXHIIYnxDIv8Jzytg+7z8bXIavhaZDX8AkTQQeGYRz5jAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=401x79>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def thresholding(img):\n",
    "    \"\"\"\n",
    "    Applies thresholding to the input image\n",
    "    \"\"\"\n",
    "    img.save(\"temp.jpg\")\n",
    "    img = cv2.imread(\"temp.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    \n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    _, im_gray_th_otsu = cv2.threshold(closing, 128, 192, cv2.THRESH_OTSU)\n",
    "\n",
    "    _, im_gray_th = cv2.threshold(im_gray_th_otsu, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    return Image.fromarray(im_gray_th)\n",
    "\n",
    "# crop_license_plate(\"IMG_2079.jpg\").show()\n",
    "# extract_license_plate_text(crop_license_plate(\"IMG_2079.jpg\")).show()\n",
    "thresholding(extract_license_plate_text(crop_license_plate(\"../test_notebooks/images/seat.webp\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
